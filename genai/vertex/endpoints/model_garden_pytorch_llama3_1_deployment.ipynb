{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "SgQ6t5bqZVlH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Llama 3.1 (Deployment)\n",
    "\n",
    "<table><tbody><tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances\">\n",
    "      <img alt=\"Workbench logo\" src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" width=\"32px\"><br> Run in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_llama3_1_deployment.ipynb\">\n",
    "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_1_deployment.ipynb\">\n",
    "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates downloading, deploying, and serving prebuilt Llama 3.1 models with [Hex-LLM](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm) or [vLLM](https://github.com/vllm-project/vllm) (standard and optimized).\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Deploy Llama 3.1 8B and 70B with Hex-LLM on TPU.\n",
    "- Deploy Llama 3.1 8B Instruct with the Fast Deployment feature.\n",
    "- Deploy Llama 3.1 8B, 70B and 405B with standard vLLM on GPU, optionally with dynamic LoRA adapters.\n",
    "- Deploy Llama 3.1 8B and 70B with optimized vLLM on GPU.\n",
    "\n",
    "### File a bug\n",
    "\n",
    "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upgrade Vertex AI SDK.\n",
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.93.1'\n",
    "! pip3 install --upgrade --quiet 'openai==1.85.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "YXFGIp1l-qtT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (75.8.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n",
      "Initializing Vertex AI API.\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "import requests\n",
    "from google import auth\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
    "    ! pip install --upgrade tensorflow\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAt6NcA5Dcjl"
   },
   "source": [
    "## Deploy prebuilt Llama 3.1 8B and 70B with Hex-LLM\n",
    "\n",
    "**Hex-LLM** is a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel (LLM) TPU serving solution built with **XLA**, which is being developed by Google Cloud.\n",
    "\n",
    "Refer to the \"Request for TPU quota\" section for TPU quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "9-5obzXZDcjl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "TPU_DEPLOYMENT_REGION = \"us-central1\"\n",
    "VERTEX_AI_MODEL_GARDEN_LLAMA_3_1 = \"gs://vertex-model-garden-restricted-us/llama3.1\"\n",
    "model_id = os.path.join(VERTEX_AI_MODEL_GARDEN_LLAMA_3_1, MODEL_ID)\n",
    "hf_model_id = \"meta-llama/\" + MODEL_ID\n",
    "\n",
    "# The pre-built serving docker images.\n",
    "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:stable\"\n",
    "use_dedicated_endpoint = True\n",
    "\n",
    "# Sets ct5lp-hightpu-4t (4 TPU chips) to deploy models.\n",
    "machine_type = \"ct5lp-hightpu-4t\"\n",
    "# Note: 1 TPU V5 chip has only one core.\n",
    "tpu_type = \"TPU_V5e\"\n",
    "\n",
    "enable_prefix_cache_hbm = True\n",
    "\n",
    "disagg_topo = None\n",
    "\n",
    "if \"8B\" in MODEL_ID:\n",
    "    tpu_count = 4\n",
    "    tpu_topo = \"1x4\"\n",
    "elif \"70B\" in MODEL_ID:\n",
    "    tpu_count = 16\n",
    "    tpu_topo = \"4x4\"\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported MODEL_ID: {MODEL_ID}\")\n",
    "\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=TPU_DEPLOYMENT_REGION,\n",
    "    accelerator_type=tpu_type,\n",
    "    accelerator_count=tpu_count,\n",
    "    is_for_training=False,\n",
    ")\n",
    "\n",
    "# Server parameters.\n",
    "tensor_parallel_size = tpu_count\n",
    "\n",
    "\n",
    "# Fraction of HBM memory allocated for KV cache after model loading. A larger value improves throughput but gives higher risk of TPU out-of-memory errors with long prompts.\n",
    "hbm_utilization_factor = 0.8\n",
    "# Maximum number of running sequences in a continuous batch.\n",
    "max_running_seqs = 256\n",
    "# Maximum context length for a request.\n",
    "max_model_len = 4096\n",
    "\n",
    "# Endpoint configurations.\n",
    "min_replica_count = 1\n",
    "max_replica_count = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deploy_model_hexllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    base_model_id: str = None,\n",
    "    data_parallel_size: int = 1,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct5lp-hightpu-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    disagg_topology: str = None,\n",
    "    hbm_utilization_factor: float = 0.6,\n",
    "    max_running_seqs: int = 256,\n",
    "    max_model_len: int = 4096,\n",
    "    enable_prefix_cache_hbm: bool = False,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \n",
    "    print(f\"model_name              :{model_name}\")\n",
    "    print(f\"model_id                :{model_id}\")\n",
    "    print(f\"publisher               :{publisher}\")\n",
    "    print(f\"publisher_model_id      :{publisher_model_id}\")\n",
    "    print(f\"base_model_id           :{base_model_id}\")\n",
    "    print(f\"data_parallel_size      :{data_parallel_size}\")\n",
    "    print(f\"tensor_parallel_size    :{tensor_parallel_size}\")\n",
    "    print(f\"machine_type            :{machine_type}\")\n",
    "    print(f\"tpu_topology            :{tpu_topology}\")\n",
    "    print(f\"disagg_topology         :{disagg_topology}\")\n",
    "    print(f\"hbm_utilization_factor  :{hbm_utilization_factor}\")\n",
    "    print(f\"max_running_seqs        :{max_running_seqs}\")\n",
    "    print(f\"max_model_len           :{max_model_len}\")\n",
    "    print(f\"enable_prefix_cache_hbm :{enable_prefix_cache_hbm}\")\n",
    "    print(f\"endpoint_id             :{endpoint_id}\")\n",
    "    print(f\"min_replica_count       :{min_replica_count}\")\n",
    "    print(f\"max_replica_count       :{max_replica_count}\")\n",
    "    print(f\"use_dedicated_endpoint  :{use_dedicated_endpoint}\")\n",
    "    \n",
    "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        print(f\"endpoint_id             :{endpoint_id}\")\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "    print(f\"base_model_id           :{base_model_id}\")\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "    print(f\"tensor_parallel_size    :{tensor_parallel_size}\")\n",
    "\n",
    "    num_hosts = int(tpu_topology.split(\"x\")[0])\n",
    "    print(f\"num_hosts               :{num_hosts}\")\n",
    "\n",
    "    # Learn more about the supported arguments and environment variables at https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm#config-server.\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--data_parallel_size={data_parallel_size}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--num_hosts={num_hosts}\",\n",
    "        f\"--hbm_utilization_factor={hbm_utilization_factor}\",\n",
    "        f\"--max_running_seqs={max_running_seqs}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "    ]\n",
    "    if disagg_topology:\n",
    "        hexllm_args.append(f\"--disagg_topo={disagg_topology}\")\n",
    "    if enable_prefix_cache_hbm and not disagg_topology:\n",
    "        hexllm_args.append(\"--enable_prefix_cache_hbm\")\n",
    "    print(f\"hexllm_args             :{hexllm_args}\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"HEX_LLM_LOG_LEVEL\": \"info\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars.update({\"HF_TOKEN\": HF_TOKEN})\n",
    "        print(f\"env_vars                :{env_vars}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.server.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        system_labels={\n",
    "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_deployment.ipynb\",\n",
    "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
    "        },\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name              :Meta-Llama-3.1-8B-Instruct-20250610-095922\n",
      "model_id                :gs://vertex-model-garden-restricted-us/llama3.1/Meta-Llama-3.1-8B-Instruct\n",
      "publisher               :meta\n",
      "publisher_model_id      :llama3_1\n",
      "base_model_id           :meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "data_parallel_size      :1\n",
      "tensor_parallel_size    :4\n",
      "machine_type            :ct5lp-hightpu-4t\n",
      "tpu_topology            :1x4\n",
      "disagg_topology         :None\n",
      "hbm_utilization_factor  :0.8\n",
      "max_running_seqs        :256\n",
      "max_model_len           :4096\n",
      "enable_prefix_cache_hbm :True\n",
      "endpoint_id             :\n",
      "min_replica_count       :1\n",
      "max_replica_count       :1\n",
      "use_dedicated_endpoint  :True\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/979398597045/locations/us-central1/endpoints/8807964449253097472/operations/1644534422063546368\n",
      "Endpoint created. Resource name: projects/979398597045/locations/us-central1/endpoints/8807964449253097472\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/979398597045/locations/us-central1/endpoints/8807964449253097472')\n",
      "base_model_id           :meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "tensor_parallel_size    :4\n",
      "num_hosts               :1\n",
      "hexllm_args             :['--host=0.0.0.0', '--port=7080', '--model=gs://vertex-model-garden-restricted-us/llama3.1/Meta-Llama-3.1-8B-Instruct', '--data_parallel_size=1', '--tensor_parallel_size=4', '--num_hosts=1', '--hbm_utilization_factor=0.8', '--max_running_seqs=256', '--max_model_len=4096', '--enable_prefix_cache_hbm']\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/979398597045/locations/us-central1/models/3790521256781021184/operations/8037394093115965440\n",
      "Model created. Resource name: projects/979398597045/locations/us-central1/models/3790521256781021184@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/979398597045/locations/us-central1/models/3790521256781021184@1')\n",
      "Deploying model to Endpoint : projects/979398597045/locations/us-central1/endpoints/8807964449253097472\n",
      "Deploy Endpoint model backing LRO: projects/979398597045/locations/us-central1/endpoints/8807964449253097472/operations/8917847820266897408\n",
      "Endpoint model deployed. Resource name: projects/979398597045/locations/us-central1/endpoints/8807964449253097472\n"
     ]
    }
   ],
   "source": [
    "LABEL = \"hexllm_tpu\"\n",
    "models[LABEL], endpoints[LABEL] = deploy_model_hexllm(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=MODEL_ID),\n",
    "    model_id=model_id,\n",
    "    publisher=\"meta\",\n",
    "    publisher_model_id=\"llama3_1\",\n",
    "    base_model_id=hf_model_id,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    machine_type=machine_type,\n",
    "    tpu_topology=tpu_topo,\n",
    "    disagg_topology=disagg_topo,\n",
    "    hbm_utilization_factor=hbm_utilization_factor,\n",
    "    max_running_seqs=max_running_seqs,\n",
    "    max_model_len=max_model_len,\n",
    "    enable_prefix_cache_hbm=enable_prefix_cache_hbm,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")\n",
    "\n",
    "model = models[LABEL]\n",
    "endpoint = endpoints[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/979398597045/locations/us-central1/endpoints/8807964449253097472\"\n",
      "display_name: \"Meta-Llama-3.1-8B-Instruct-20250610-095922-endpoint\"\n",
      "deployed_models {\n",
      "  id: \"6420641031351435264\"\n",
      "  model: \"projects/979398597045/locations/us-central1/models/3790521256781021184\"\n",
      "  display_name: \"Meta-Llama-3.1-8B-Instruct-20250610-095922\"\n",
      "  create_time {\n",
      "    seconds: 1749549567\n",
      "    nanos: 740805000\n",
      "  }\n",
      "  dedicated_resources {\n",
      "    machine_spec {\n",
      "      machine_type: \"ct5lp-hightpu-4t\"\n",
      "    }\n",
      "    min_replica_count: 1\n",
      "    max_replica_count: 1\n",
      "  }\n",
      "  model_version_id: \"1\"\n",
      "  status {\n",
      "    available_replica_count: 1\n",
      "  }\n",
      "}\n",
      "traffic_split {\n",
      "  key: \"6420641031351435264\"\n",
      "  value: 100\n",
      "}\n",
      "etag: \"AMEw9yPmcJFndG1suqtWngndiwzFJ7v7vNmuj_qtWzs-45khQqDqXM3Tz7NNVX9g-w==\"\n",
      "create_time {\n",
      "  seconds: 1749549563\n",
      "  nanos: 74893000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1749550948\n",
      "  nanos: 1787000\n",
      "}\n",
      "dedicated_endpoint_enabled: true\n",
      "dedicated_endpoint_dns: \"8807964449253097472.us-central1-979398597045.prediction.vertexai.goog\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(endpoints[\"hexllm_tpu\"].gca_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原生SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "cEq8oadxDcjl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A car is a road vehicle, typically with four wheels, powered by an internal combustion engine or an electric motor. Cars are used for transportation, recreation, and other purposes. They come in various shapes, sizes, and models, ranging from small hatch\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a car?\" \n",
    "max_tokens = 50\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = 1\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]\n",
    "response = endpoints[\"hexllm_tpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat completion推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "3d665186ff73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_URL : https://8807964449253097472.us-central1-979398597045.prediction.vertexai.goog/v1beta1/projects/ali-icbu-gpu-project/locations/us-central1/endpoints/8807964449253097472\n",
      "ChatCompletion(id=None, choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm just a language model, I don't have emotions or experiences like humans do, so I don't have good or bad days. However, I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How's your day going?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]))], created=None, model='', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=59, prompt_tokens=42, total_tokens=101, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# @title Chat completion\n",
    "\n",
    "temp_region = REGION\n",
    "REGION = TPU_DEPLOYMENT_REGION\n",
    "\n",
    "if use_dedicated_endpoint:\n",
    "    DEDICATED_ENDPOINT_DNS = endpoints[\"hexllm_tpu\"].gca_resource.dedicated_endpoint_dns\n",
    "    ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
    "        PROJECT_ID, REGION, endpoints[\"hexllm_tpu\"].name)\n",
    "\n",
    "! pip install -qU openai google-auth requests\n",
    "\n",
    "user_message = \"How is your day going?\"\n",
    "max_tokens = 500\n",
    "temperature = 1.0\n",
    "stream = False\n",
    "\n",
    "\n",
    "import google.auth\n",
    "import openai\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "BASE_URL = (\n",
    "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    ")\n",
    "try:\n",
    "    if use_dedicated_endpoint:\n",
    "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(f'BASE_URL : {BASE_URL}')\n",
    "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
    "\n",
    "model_response = client.chat.completions.create(\n",
    "    model=\"\",\n",
    "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    stream=stream,\n",
    ")\n",
    "\n",
    "if stream:\n",
    "    usage = None\n",
    "    contents = []\n",
    "    for chunk in model_response:\n",
    "        if chunk.usage is not None:\n",
    "            usage = chunk.usage\n",
    "            continue\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "        contents.append(chunk.choices[0].delta.content)\n",
    "    print(f\"\\n\\n{usage}\")\n",
    "else:\n",
    "    print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-XybZjtgF9M"
   },
   "source": [
    "## Deploy prebuilt Llama 3.1 8B, 70B and 405B with standard vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "E8OiHHNNE_wj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerator_type : NVIDIA_L4\n",
      "machine_type     : g2-standard-12\n",
      "accelerator_count: 1\n",
      "max_loras        : 5\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id = os.path.join(VERTEX_AI_MODEL_GARDEN_LLAMA_3_1, base_model_name)\n",
    "ENABLE_DYNAMIC_LORA = True\n",
    "version_id = base_model_name.lower()[5:]\n",
    "hf_model_id = \"meta-llama/\" + base_model_name\n",
    "PUBLISHER_MODEL_NAME = f\"publishers/meta/models/llama3_1@{version_id}\"\n",
    "\n",
    "# The pre-built serving docker images.\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20241210_0916_RC00\"\n",
    "\n",
    "use_dedicated_endpoint = True\n",
    "if \"8b\" in base_model_name.lower():\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    machine_type = \"g2-standard-12\"\n",
    "    accelerator_count = 1\n",
    "    max_loras = 5\n",
    "elif \"70b\" in base_model_name.lower():\n",
    "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
    "    machine_type = \"a3-highgpu-4g\"\n",
    "    accelerator_count = 4\n",
    "    max_loras = 1\n",
    "elif \"405b\" in base_model_name.lower():\n",
    "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
    "    machine_type = \"a3-highgpu-8g\"\n",
    "    accelerator_count = 8\n",
    "    max_loras = 1\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Recommended GPU setting not found for: {accelerator_type} and {base_model_name}.\"\n",
    "    )\n",
    "    \n",
    "print(f'accelerator_type : {accelerator_type}')\n",
    "print(f'machine_type     : {machine_type}')\n",
    "print(f'accelerator_count: {accelerator_count}')\n",
    "print(f'max_loras        : {max_loras}')\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "DsRuuOjjLzq-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_memory_utilization = 0.95\n",
    "max_model_len = 8192  # Maximum context length.\n",
    "\n",
    "# Enable automatic prefix caching using GPU HBM\n",
    "enable_prefix_cache = True\n",
    "# Setting this value >0 will use the idle host memory for a second-tier prefix kv\n",
    "# cache beneath the HBM cache. It only has effect if enable_prefix_cache=True.\n",
    "# The range of this value: [0, 1)\n",
    "# Setting host_prefix_kv_cache_utilization_target to 0 will disable the host memory prefix kv cache.\n",
    "host_prefix_kv_cache_utilization_target = 0.7\n",
    "\n",
    "is_spot = False\n",
    "\n",
    "min_replica_count = 1\n",
    "max_replica_count = 1\n",
    "required_replica_count = 1\n",
    "\n",
    "# Set the target of GPU duty cycle or CPU usage between 1 and 100 for auto-scaling.\n",
    "autoscale_by_gpu_duty_cycle_target = 0\n",
    "autoscale_by_cpu_usage_target = 0\n",
    "\n",
    "# Note: GPU duty cycle is not the most accurate metric for scaling workloads. More advanced auto-scaling metrics are coming soon. See [the public doc](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#autoscaling) for more details.\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    publisher: str,\n",
    "    publisher_model_id: str,\n",
    "    base_model_id: str = None,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    max_model_len: int = 4096,\n",
    "    dtype: str = \"auto\",\n",
    "    enable_trust_remote_code: bool = False,\n",
    "    enforce_eager: bool = False,\n",
    "    enable_lora: bool = False,\n",
    "    enable_chunked_prefill: bool = False,\n",
    "    enable_prefix_cache: bool = False,\n",
    "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
    "    max_loras: int = 1,\n",
    "    max_cpu_loras: int = 8,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    max_num_seqs: int = 256,\n",
    "    model_type: str = None,\n",
    "    enable_llama_tool_parser: bool = False,\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    required_replica_count: int = 1,\n",
    "    autoscale_by_gpu_duty_cycle_target: int = 0,\n",
    "    autoscale_by_cpu_usage_target: int = 0,\n",
    "    is_spot: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "    )\n",
    "    \n",
    "    print(f\"model_name                              :{model_name}\")\n",
    "    print(f\"model_id                                :{model_id}\")\n",
    "    print(f\"publisher                               :{publisher}\")\n",
    "    print(f\"publisher_model_id                      :{publisher_model_id}\")\n",
    "    print(f\"base_model_id                           :{base_model_id}\")\n",
    "    print(f\"machine_type                            :{machine_type}\")\n",
    "    print(f\"accelerator_type                        :{accelerator_type}\")\n",
    "    print(f\"accelerator_count                       :{accelerator_count}\")\n",
    "    print(f\"gpu_memory_utilization                  :{gpu_memory_utilization}\")\n",
    "    print(f\"max_model_len                           :{max_model_len}\")\n",
    "    print(f\"dtype                                   :{dtype}\")\n",
    "    print(f\"enable_trust_remote_code                :{enable_trust_remote_code}\")\n",
    "    print(f\"enforce_eager                           :{enforce_eager}\")\n",
    "    print(f\"enable_lora                             :{enable_lora}\")\n",
    "    print(f\"enable_chunked_prefill                  :{enable_chunked_prefill}\")\n",
    "    print(f\"enable_prefix_cache                     :{enable_prefix_cache}\")\n",
    "    print(f\"host_prefix_kv_cache_utilization_target :{host_prefix_kv_cache_utilization_target}\")\n",
    "    print(f\"max_loras                               :{max_loras}\")\n",
    "    print(f\"max_cpu_loras                           :{max_cpu_loras}\")\n",
    "    print(f\"use_dedicated_endpoint                  :{use_dedicated_endpoint}\")\n",
    "    print(f\"max_num_seqs                            :{max_num_seqs}\")\n",
    "    print(f\"model_type                              :{model_type}\")\n",
    "    print(f\"enable_llama_tool_parser                :{enable_llama_tool_parser}\")\n",
    "    print(f\"min_replica_count                       :{min_replica_count}\")\n",
    "    print(f\"max_replica_count                       :{max_replica_count}\")\n",
    "    print(f\"required_replica_count                  :{required_replica_count}\")\n",
    "    print(f\"autoscale_by_gpu_duty_cycle_target      :{autoscale_by_gpu_duty_cycle_target}\")\n",
    "    print(f\"autoscale_by_cpu_usage_target           :{autoscale_by_cpu_usage_target}\")\n",
    "    print(f\"is_spot                                 :{is_spot}\")\n",
    "\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
    "    vllm_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=8080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        f\"--max-loras={max_loras}\",\n",
    "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
    "        f\"--max-num-seqs={max_num_seqs}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    if enable_trust_remote_code:\n",
    "        vllm_args.append(\"--trust-remote-code\")\n",
    "\n",
    "    if enforce_eager:\n",
    "        vllm_args.append(\"--enforce-eager\")\n",
    "\n",
    "    if enable_lora:\n",
    "        vllm_args.append(\"--enable-lora\")\n",
    "\n",
    "    if enable_chunked_prefill:\n",
    "        vllm_args.append(\"--enable-chunked-prefill\")\n",
    "\n",
    "    if enable_prefix_cache:\n",
    "        vllm_args.append(\"--enable-prefix-caching\")\n",
    "\n",
    "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
    "        vllm_args.append(\n",
    "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
    "        )\n",
    "\n",
    "    if model_type:\n",
    "        vllm_args.append(f\"--model-type={model_type}\")\n",
    "\n",
    "    if enable_llama_tool_parser:\n",
    "        if \"Llama-4\" not in model_id:\n",
    "            vllm_args.append(\"--enable-auto-tool-choice\")\n",
    "            vllm_args.append(\"--tool-call-parser=vertex-llama-3\")\n",
    "        else:\n",
    "            vllm_args.append(\"--enable-auto-tool-choice\")\n",
    "            vllm_args.append(\"--tool-call-parser=llama3_json\")\n",
    "            \n",
    "    print(f\"vllm_args                               :{vllm_args}\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "        print(f\"env_vars                                :{env_vars}\")\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        model_garden_source_model_name=(\n",
    "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "\n",
    "    creds, _ = auth.default()\n",
    "    auth_req = auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "\n",
    "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {creds.token}\",\n",
    "    }\n",
    "    data = {\n",
    "        \"deployedModel\": {\n",
    "            \"model\": model.resource_name,\n",
    "            \"displayName\": model_name,\n",
    "            \"dedicatedResources\": {\n",
    "                \"machineSpec\": {\n",
    "                    \"machineType\": machine_type,\n",
    "                    \"acceleratorType\": accelerator_type,\n",
    "                    \"acceleratorCount\": accelerator_count,\n",
    "                },\n",
    "                \"minReplicaCount\": min_replica_count,\n",
    "                \"requiredReplicaCount\": required_replica_count,\n",
    "                \"maxReplicaCount\": max_replica_count,\n",
    "            },\n",
    "            \"system_labels\": {\n",
    "                \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_deployment.ipynb\",\n",
    "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    if is_spot:\n",
    "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
    "    if autoscale_by_gpu_duty_cycle_target > 0 or autoscale_by_cpu_usage_target > 0:\n",
    "        data[\"deployedModel\"][\"dedicatedResources\"][\"autoscalingMetricSpecs\"] = []\n",
    "        if autoscale_by_gpu_duty_cycle_target > 0:\n",
    "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
    "                \"autoscalingMetricSpecs\"\n",
    "            ].append(\n",
    "                {\n",
    "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle\",\n",
    "                    \"target\": autoscale_by_gpu_duty_cycle_target,\n",
    "                }\n",
    "            )\n",
    "        if autoscale_by_cpu_usage_target > 0:\n",
    "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
    "                \"autoscalingMetricSpecs\"\n",
    "            ].append(\n",
    "                {\n",
    "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/cpu/utilization\",\n",
    "                    \"target\": autoscale_by_cpu_usage_target,\n",
    "                }\n",
    "            )\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    print(f\"Deploy Model response: {response.json()}\")\n",
    "    if response.status_code != 200 or \"name\" not in response.json():\n",
    "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
    "    common_util.poll_and_wait(response.json()[\"name\"], REGION, 7200)\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    return model, endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/979398597045/locations/us-central1/endpoints/991967295951601664/operations/8971609540818632704\n",
      "Endpoint created. Resource name: projects/979398597045/locations/us-central1/endpoints/991967295951601664\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/979398597045/locations/us-central1/endpoints/991967295951601664')\n",
      "model_name                              :llama3-1-serve-20250610-103803\n",
      "model_id                                :gs://vertex-model-garden-restricted-us/llama3.1/Meta-Llama-3.1-8B-Instruct\n",
      "publisher                               :meta\n",
      "publisher_model_id                      :llama3_1\n",
      "base_model_id                           :meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "machine_type                            :g2-standard-12\n",
      "accelerator_type                        :NVIDIA_L4\n",
      "accelerator_count                       :1\n",
      "gpu_memory_utilization                  :0.95\n",
      "max_model_len                           :8192\n",
      "dtype                                   :auto\n",
      "enable_trust_remote_code                :False\n",
      "enforce_eager                           :True\n",
      "enable_lora                             :True\n",
      "enable_chunked_prefill                  :False\n",
      "enable_prefix_cache                     :True\n",
      "host_prefix_kv_cache_utilization_target :0.7\n",
      "max_loras                               :5\n",
      "max_cpu_loras                           :8\n",
      "use_dedicated_endpoint                  :True\n",
      "max_num_seqs                            :256\n",
      "model_type                              :None\n",
      "enable_llama_tool_parser                :True\n",
      "min_replica_count                       :1\n",
      "max_replica_count                       :1\n",
      "required_replica_count                  :1\n",
      "autoscale_by_gpu_duty_cycle_target      :0\n",
      "autoscale_by_cpu_usage_target           :0\n",
      "is_spot                                 :False\n",
      "vllm_args                               :['python', '-m', 'vllm.entrypoints.api_server', '--host=0.0.0.0', '--port=8080', '--model=gs://vertex-model-garden-restricted-us/llama3.1/Meta-Llama-3.1-8B-Instruct', '--tensor-parallel-size=1', '--swap-space=16', '--gpu-memory-utilization=0.95', '--max-model-len=8192', '--dtype=auto', '--max-loras=5', '--max-cpu-loras=8', '--max-num-seqs=256', '--disable-log-stats', '--enforce-eager', '--enable-lora', '--enable-prefix-caching', '--host-prefix-kv-cache-utilization-target=0.7', '--enable-auto-tool-choice', '--tool-call-parser=vertex-llama-3']\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/979398597045/locations/us-central1/models/2709657346212102144/operations/8588803572492140544\n",
      "Model created. Resource name: projects/979398597045/locations/us-central1/models/2709657346212102144@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/979398597045/locations/us-central1/models/2709657346212102144@1')\n",
      "Deploying llama3-1-serve-20250610-103803 on g2-standard-12 with 1 NVIDIA_L4 GPU(s).\n",
      "Deploy Model response: {'name': 'projects/979398597045/locations/us-central1/operations/180583068191424512', 'metadata': {'@type': 'type.googleapis.com/google.cloud.aiplatform.ui.DeployModelOperationMetadata', 'genericMetadata': {'createTime': '2025-06-10T10:38:08.487293Z', 'updateTime': '2025-06-10T10:38:08.487293Z', 'state': 'RUNNING', 'worksOn': ['projects/979398597045/locations/us-central1/endpoints/991967295951601664']}}}\n",
      "Still waiting for operation... Elapsed time in seconds: 1502  endpoint_name: 991967295951601664\n"
     ]
    }
   ],
   "source": [
    "LABEL = \"vllm_gpu\"\n",
    "models[LABEL], endpoints[LABEL] = deploy_model_vllm(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3_1-serve\"),\n",
    "    model_id=model_id,\n",
    "    publisher=\"meta\",\n",
    "    publisher_model_id=\"llama3_1\",\n",
    "    base_model_id=hf_model_id,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    "    max_loras=max_loras,\n",
    "    enforce_eager=True,\n",
    "    enable_lora=ENABLE_DYNAMIC_LORA,\n",
    "    enable_chunked_prefill=not ENABLE_DYNAMIC_LORA,\n",
    "    enable_prefix_cache=enable_prefix_cache,\n",
    "    host_prefix_kv_cache_utilization_target=host_prefix_kv_cache_utilization_target,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    "    enable_llama_tool_parser=True,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    required_replica_count=required_replica_count,\n",
    "    autoscale_by_gpu_duty_cycle_target=autoscale_by_gpu_duty_cycle_target,\n",
    "    autoscale_by_cpu_usage_target=autoscale_by_cpu_usage_target,\n",
    "    is_spot=is_spot,\n",
    ")\n",
    "\n",
    "model = models[LABEL]\n",
    "endpoint = endpoints[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/979398597045/locations/us-central1/endpoints/991967295951601664\"\n",
      "display_name: \"llama3-1-serve-20250610-103803-endpoint\"\n",
      "etag: \"AMEw9yNe6PBFD7WR9baQldSKtlqTImFGRPr03oGFTeDkpe17vbXBBvF9cC9c1APSPrEM\"\n",
      "create_time {\n",
      "  seconds: 1749551884\n",
      "  nanos: 55702000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1749551885\n",
      "  nanos: 434051000\n",
      "}\n",
      "dedicated_endpoint_enabled: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(endpoint.gca_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原生SDK推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "rDHsCOqvFYBi",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car?\n",
      "Output:\n",
      " A car is a road vehicle, typically with four wheels, powered by an internal combustion engine or an electric motor. Cars are used for transportation, and they come in a wide range of shapes, sizes, and styles. From compact sedans to luxury SUVs, cars are an essential part of modern life.\n",
      "The first cars were invented in the late 19th century, and they were powered by steam engines. However, it wasn't until the early 20th century that cars became widely available and affordable for the general public. Since then, cars have evolved significantly, with advances in technology, design, and safety features.\n",
      "Today, cars are a ubiquitous part of modern life, with millions of vehicles on the road worldwide. They are used for commuting, road trips, and other forms of transportation. Cars also come with a range of features, such as air conditioning, GPS navigation, and infotainment systems, making them a comfortable and convenient mode of transportation.\n",
      "Types of Cars:\n",
      "There are many types of cars, including:\n",
      "1. Sedans: These are the most common type of car, with a fixed roof and a separate trunk.\n",
      "2. Hatchbacks: These cars have a rear door that swings upwards, providing access to the cargo area.\n",
      "3. SUVs (Sport Utility Vehicles): These cars are designed for off-road driving and have a higher ground clearance than sedans.\n",
      "4. Trucks: These cars are designed for hauling heavy loads and have a larger cargo area than sedans.\n",
      "5. Electric Cars: These cars are powered by electric motors and do not produce emissions.\n",
      "6. Hybrid Cars: These cars combine a gasoline engine with an electric motor to improve fuel efficiency.\n",
      "7. Sports Cars: These cars are designed for speed and performance, with a focus on handling and acceleration.\n",
      "8. Luxury Cars: These cars are high-end vehicles with advanced features and premium materials.\n",
      "9. Minivans: These cars are designed for families, with a sliding door and a spacious interior.\n",
      "10. Convertibles: These cars have a retractable roof, providing an open-air driving experience.\n",
      "Car Features:\n",
      "Cars come with a range of features, including:\n",
      "1. Air conditioning: This feature provides a comfortable temperature inside the car.\n",
      "2. GPS navigation: This feature provides turn-by-turn directions and helps drivers navigate unfamiliar areas.\n",
      "3. Infotainment systems: These systems provide entertainment, such as music and video, and also offer features like Bluetooth connectivity and voice commands.\n",
      "4. Safety\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"What is a car?\" \n",
    "max_tokens = 500\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = 1\n",
    "# Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = False \n",
    "lora_id = \"\"\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instance = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": max_tokens,\n",
    "    \"temperature\": temperature,\n",
    "    \"top_p\": top_p,\n",
    "    \"top_k\": top_k,\n",
    "    \"raw_response\": raw_response,\n",
    "}\n",
    "if lora_id:\n",
    "    instance[\"dynamic-lora\"] = lora_id\n",
    "instances = [instance]\n",
    "response = endpoints[\"vllm_gpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI SDK推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "LSG9ITWTbTb7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-6de8c92690954cdeb443392eaaf8e210', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm just a language model, so I don't have emotions or experiences like a human would. However, I'm functioning properly and ready to help with any questions or tasks you may have. It's always great to have someone to chat with, so feel free to ask me anything or start a conversation on a topic that interests you.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1749553572, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=69, prompt_tokens=42, total_tokens=111, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "# @title Chat completion\n",
    "\n",
    "if use_dedicated_endpoint:\n",
    "    DEDICATED_ENDPOINT_DNS = endpoints[LABEL].gca_resource.dedicated_endpoint_dns\n",
    "ENDPOINT_RESOURCE_NAME = endpoints[LABEL].resource_name\n",
    "\n",
    "\n",
    "! pip install -qU openai google-auth requests\n",
    "\n",
    "user_message = \"How is your day going?\" \n",
    "max_tokens = 500\n",
    "temperature = 1.0\n",
    "stream = False\n",
    "\n",
    "# Now we can send a request.\n",
    "\n",
    "import google.auth\n",
    "import openai\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "BASE_URL = (\n",
    "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    ")\n",
    "try:\n",
    "    if use_dedicated_endpoint:\n",
    "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
    "\n",
    "model_response = client.chat.completions.create(\n",
    "    model=\"\",\n",
    "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    stream=stream,\n",
    ")\n",
    "\n",
    "if stream:\n",
    "    usage = None\n",
    "    contents = []\n",
    "    for chunk in model_response:\n",
    "        if chunk.usage is not None:\n",
    "            usage = chunk.usage\n",
    "            continue\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "        contents.append(chunk.choices[0].delta.content)\n",
    "    print(f\"\\n\\n{usage}\")\n",
    "else:\n",
    "    print(model_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JETd33jIDcjm"
   },
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# @title Delete the models and endpoints\n",
    "\n",
    "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
    "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "for endpoint in endpoints.values():\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "for model in models.values():\n",
    "    model.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_1_deployment.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
